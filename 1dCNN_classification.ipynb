{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d734e7",
   "metadata": {},
   "source": [
    "# 1D CNN text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3537a834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0251f246",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e0b517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_dataset(path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            texts.append(row[\"text\"])\n",
    "            labels.append(int(row[\"label\"]))\n",
    "\n",
    "    return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1292af0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, samples, labels, max_len=256):\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, s):\n",
    "        # byte encoding, values 0–255\n",
    "        x = list(s.encode(\"utf-8\"))[:self.max_len]\n",
    "        # pad with zeros\n",
    "        pad_len = self.max_len - len(x)\n",
    "        x = x + [0] * pad_len\n",
    "        return torch.tensor(x, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.encode(self.samples[idx]), torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "\n",
    "train_texts, train_labels = load_csv_dataset(\"dataset/train_dataset.csv\")\n",
    "train_loader = DataLoader(\n",
    "    TextDataset(train_texts, train_labels),\n",
    "    batch_size=16,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_texts, val_labels = load_csv_dataset(\"dataset/val_dataset.csv\")\n",
    "val_loader = DataLoader(\n",
    "    TextDataset(val_texts, val_labels),\n",
    "    batch_size=16,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9bcc0",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "622ce6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        embedding_dim=32,\n",
    "        num_filters=128,\n",
    "        kernel_sizes=(3, 5, 7),\n",
    "        dropout=0.2,\n",
    "        num_classes=1  # binary classification\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Character embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "\n",
    "        # Multiple convolution branches\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k\n",
    "            ) for k in kernel_sizes\n",
    "        ])\n",
    "\n",
    "        # Final classifier\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(num_filters * len(kernel_sizes), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len) - tensor of character IDs\n",
    "        \"\"\"\n",
    "        embedded = self.embedding(x)  # (batch, seq, embed)\n",
    "        embedded = embedded.transpose(1, 2)  # (batch, embed, seq)\n",
    "\n",
    "        # Apply conv → ReLU → global max pool\n",
    "        conv_results = []\n",
    "        for conv in self.convs:\n",
    "            c = conv(embedded)                 # (batch, filters, seq')\n",
    "            c = F.relu(c)\n",
    "            c = F.max_pool1d(c, kernel_size=c.size(2))  # global max pool\n",
    "            conv_results.append(c.squeeze(2))\n",
    "\n",
    "        # Concatenate features\n",
    "        features = torch.cat(conv_results, dim=1)\n",
    "\n",
    "        out = self.dropout(features)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        # Output logits (use BCEWithLogitsLoss)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ad561b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8b77b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c9e9343",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharCNN(vocab_size=256)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7e208b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: time 25.36s, loss = 33.2913\n",
      "Epoch 2: time 25.37s, loss = 0.4080\n",
      "Epoch 3: time 27.16s, loss = 0.1158\n",
      "Epoch 4: time 26.95s, loss = 0.0482\n",
      "Epoch 5: time 30.80s, loss = 0.0224\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    t0 = time.time()\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits.squeeze(1), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: time {(time.time()-t0):.2f}s, loss = {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfedfe",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ef2e003e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharCNN(\n",
      "  (embedding): Embedding(256, 32, padding_idx=0)\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv1d(32, 128, kernel_size=(3,), stride=(1,))\n",
      "    (1): Conv1d(32, 128, kernel_size=(5,), stride=(1,))\n",
      "    (2): Conv1d(32, 128, kernel_size=(7,), stride=(1,))\n",
      "  )\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=384, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed247932",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "22c09e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, text):\n",
    "    model.eval()\n",
    "    x = torch.tensor([c for c in text.encode()][:256] + [0]*(256-len(text)), dtype=torch.long)\n",
    "    x = x.unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        p = torch.sigmoid(model(x)).item()\n",
    "    return p  # close to 1 = real text, close to 0 = random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "30678d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n",
      "Precision: 1.0000\n",
      "Recall: 1.0000\n",
      "F1 Score: 1.0000\n"
     ]
    }
   ],
   "source": [
    "TP,TN,FP,FN = 0,0,0,0\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for xs, ys in val_loader:\n",
    "        logits = model(xs)\n",
    "        probs = torch.sigmoid(logits.squeeze(1))\n",
    "        preds = (probs > 0.5).long()\n",
    "        targets = ys.long()\n",
    "\n",
    "        TP += int(((preds == 1) & (targets == 1)).sum().item())\n",
    "        TN += int(((preds == 0) & (targets == 0)).sum().item())\n",
    "        FP += int(((preds == 1) & (targets == 0)).sum().item())\n",
    "        FN += int(((preds == 0) & (targets == 1)).sum().item())\n",
    "\n",
    "\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0facda60",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5148c032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9866163730621338\n",
      "0.1002049371600151\n"
     ]
    }
   ],
   "source": [
    "print(predict(model, \"This is a sample of real text.\"))  # Expected: close to 1\n",
    "print(predict(model, \"gif uli afek sms\"))  # Expected: close to 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
