{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db447ef1",
   "metadata": {},
   "source": [
    "# nGram classificator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a32772bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, math, random, collections, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6a2158",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a37a8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramCharLM:\n",
    "    def __init__(self, ngram_size=3, n_thresholds=20):\n",
    "        self.ngram_size = max(1, ngram_size)\n",
    "        self.n_thresholds = max(1, n_thresholds)\n",
    "        self.counts = collections.defaultdict(collections.Counter)\n",
    "        self.context_totals = collections.Counter()\n",
    "        self.vocab = set()\n",
    "        self.thresholds = {i:(None,None) for i in range(3,n_thresholds)} # prob. there is no need for storing all thresholds\n",
    "\n",
    "    def _normalize(self, s):\n",
    "        return s.replace(\"\\r\",\" \").replace(\"\\n\",\" \").lower()\n",
    "\n",
    "    def train(self, corpus, samples=200):\n",
    "        self.train_positive(corpus)\n",
    "        self.train_negative(samples)\n",
    "\n",
    "    def train_positive(self, corpus):\n",
    "        s = self._normalize(corpus)\n",
    "        self.vocab.update(set(s))\n",
    "        padded = (\" \" * (self.ngram_size- 1)) + s\n",
    "        for i in range(len(padded) - self.ngram_size+ 1):\n",
    "            ctx = padded[i:i + self.ngram_size- 1]\n",
    "            ch = padded[i + self.ngram_size- 1]\n",
    "            self.counts[ctx][ch] += 1\n",
    "            self.context_totals[ctx] += 1\n",
    "    \n",
    "    def train_negative(self, samples=200):\n",
    "        def generate_random_string(length, chars):\n",
    "            return \"\".join(random.choice(list(chars)) for _ in range(length))\n",
    "        chars = self.vocab or set(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "        for length in self.thresholds.keys():\n",
    "            log_probs = [self.score(generate_random_string(length, chars)) for _ in range(samples)]\n",
    "            mean = sum(log_probs) / len(log_probs)\n",
    "            variance = sum((x - mean) ** 2 for x in log_probs) / len(log_probs)\n",
    "            stddev = math.sqrt(variance)\n",
    "            self.thresholds[length] = (mean, stddev)\n",
    "\n",
    "    def char_prob(self, context, ch):\n",
    "        # Laplace (add-one) smoothing\n",
    "        ctx = context[-(self.ngram_size - 1):] if self.ngram_size > 1 else \"\"\n",
    "        V = len(self.vocab) or 1\n",
    "        count = self.counts.get(ctx, {}).get(ch, 0)\n",
    "        total = self.context_totals.get(ctx, 0)\n",
    "        return (count + 1) / (total + V)\n",
    "\n",
    "    def score(self, text):\n",
    "        s = self._normalize(text)\n",
    "        padded = (\" \" * (self.ngram_size- 1)) + s\n",
    "        logp = 0.0\n",
    "        n_chars = 0\n",
    "        for i in range(len(padded) - self.ngram_size+ 1):\n",
    "            ctx = padded[i:i + self.ngram_size- 1]\n",
    "            ch = padded[i + self.ngram_size- 1]\n",
    "            p = self.char_prob(ctx, ch)\n",
    "            logp += math.log(p)\n",
    "            n_chars += 1\n",
    "        return logp / max(1, n_chars)  # average log-prob per char\n",
    "\n",
    "    def predict(self, text, alpha=2.0):\n",
    "        score = self.score(text)\n",
    "        length = len(text)\n",
    "        if length < 3:\n",
    "            raise ValueError(\"Text too short to evaluate\")\n",
    "        mean, stddev = self.thresholds.get(length, (None, None))\n",
    "        if mean is None or stddev is None:\n",
    "            raise ValueError(\"No threshold available for this text length\")\n",
    "        threshold = mean + alpha * stddev\n",
    "        return score > threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40f807d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "934ca987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{3: (-2.9497694227157973, 0.04877667244338073), 4: (-2.9492391291861733, 0.038477788758329336), 5: (-2.94378612197902, 0.03580025998825171), 6: (-2.9476390791795954, 0.02331480188620268), 7: (-2.946264987911756, 0.021775978898016322), 8: (-2.9444321021031605, 0.023197934975401457), 9: (-2.9447894517136213, 0.01878430640176478), 10: (-2.946359039174334, 0.015391115503331733), 11: (-2.9447257294323155, 0.016439377146958183), 12: (-2.943899516219111, 0.016214840636483607), 13: (-2.9449284808860936, 0.012597469542271542), 14: (-2.945351983539099, 0.011813869767967746), 15: (-2.9457190191717033, 0.009325920754481149), 16: (-2.945639016671375, 0.008743050707326078), 17: (-2.9446245234561252, 0.011902161105045798), 18: (-2.945683971916986, 0.008207278157979822), 19: (-2.945618445982746, 0.007003103046113378), 20: (-2.9450780822273015, 0.007621592614305759)}\n"
     ]
    }
   ],
   "source": [
    "ngram_size = 0\n",
    "train_dataset_path = \"dataset/wikipedia_clean_corpus.txt\"\n",
    "    \n",
    "model = NGramCharLM(ngram_size, n_thresholds=21)\n",
    "model.train(train_dataset_path)\n",
    "print(model.thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738b2c2f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe703d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 3, TN: 96, FP: 3, FN: 71\n",
      "Accuracy: 0.5722543352601156\n",
      "Precision: 0.5\n",
      "Recall: 0.04054054054054054\n",
      "F1 Score: 0.075\n"
     ]
    }
   ],
   "source": [
    "def evaluate(alpha=0.0):\n",
    "    with open(\"dataset/eval_dataset.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "        TP, TN, FP, FN = 0, 0, 0, 0\n",
    "        for line in lines:\n",
    "            l = line.strip().split(\"  \")\n",
    "            if len(l) < 2 or len(l[0]) < 3:\n",
    "                continue  # skip too short texts\n",
    "            text, label = l[0], l[1]\n",
    "            prediction = model.predict(text, alpha)\n",
    "            if prediction and label == \"1\":\n",
    "                TP += 1\n",
    "            elif prediction and label == \"0\":\n",
    "                FP += 1\n",
    "            elif not prediction and label == \"0\":\n",
    "                TN += 1\n",
    "            elif not prediction and label == \"1\":\n",
    "                FN += 1\n",
    "\n",
    "        print(f\"TP: {TP}, TN: {TN}, FP: {FP}, FN: {FN}\")\n",
    "        accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "        precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "        recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1 Score: {(2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0}\")\n",
    "\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc54fbf6",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76aaedf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: LIKELY RANDOM\n"
     ]
    }
   ],
   "source": [
    "text = 'asdgnlabkegrSV'\n",
    "res = model.predict(text, alpha=2)\n",
    "\n",
    "print(\"Result: {}\".format(\"LIKELY REAL\" if res else \"LIKELY RANDOM\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
